#!/usr/bin/env python3

import os
import argparse
import warnings
import logging
import pandas as pd
import tensorflow as tf

from datetime import datetime

from vbfml.util import (
    vbfml_path,
    git_rev_parse,
    git_diff,
    ModelConfiguration,
    ModelFactory,
)
from vbfml.helpers.deployment import pack_repo
from vbfml.helpers.condor import condor_submit
from vbfml.training.util import do_setup

pjoin = os.path.join

# Ignore pandas performance warnings for now
warnings.filterwarnings("ignore", category=pd.errors.PerformanceWarning)

# Tensorflow can be more silent
logging.getLogger("tensorflow").setLevel(logging.ERROR)


def parse_cli():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-i",
        "--input_dir",
        default=None,
        help="Path to the directory containing input ROOT files.",
    )
    parser.add_argument(
        "-m",
        "--model_config",
        default=None,
        help="Path to the model configuration yml file.",
    )
    parser.add_argument(
        "-n",
        "--num_epochs",
        type=int,
        default=20,
        help="Number of iterations through the whole training set.",
    )
    parser.add_argument(
        "--name", default=None, help="Job name specifying this particular submission."
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="If a submission directory exists, whether to overwrite. Defaults to False.",
    )
    parser.add_argument(
        "--jobs", type=int, default=1, help="Number of cores to request."
    )
    parser.add_argument(
        "--memory", type=int, default=None, help="Maximum amount of memory to request."
    )
    parser.add_argument("--dryrun", action="store_true", help="Dry run flag.")
    args = parser.parse_args()
    return args


def do_submit(args):
    """Submission operation."""
    import htcondor

    # Create the submission directory
    if args.name:
        subdir = os.path.abspath(pjoin("./submission", args.name))
        if os.path.exists(subdir) and not args.overwrite:
            raise RuntimeError(
                f"Will not use existing submission directory unless --overwrite is specified: {subdir}"
            )
    else:
        timetag = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        subdir = os.path.abspath(pjoin("./submission", timetag))

    if not os.path.exists(subdir):
        os.makedirs(subdir)

    # Set up the training area
    scale_events = {args.scale_events[0]: args.scale_events[1]}
    do_setup(
        output_directory=subdir,
        input_dir=os.path.abspath(args.input_dir),
        model_config=args.model_config,
        scale_events=scale_events,
    )

    # Set up the model and save into the training directory
    mconfig = ModelConfiguration(args.model_config)
    model = ModelFactory.build(mconfig)

    optimizer = tf.keras.optimizers.Adam(
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-07,
        amsgrad=False,
        name="Adam",
    )
    model.compile(
        loss="categorical_crossentropy",
        optimizer=optimizer,
        weighted_metrics=["categorical_accuracy"],
    )
    model.summary()

    model.save(pjoin(subdir, "models/latest"), include_optimizer=True)

    # Repo version information
    with open(pjoin(subdir, "version.txt"), "w") as f:
        f.write(git_rev_parse() + "\n")
        f.write(git_diff() + "\n")

    # List of input files/directories we want to ship to the execution machine
    input_files = []

    # Pack the repository for deployment to execution machine
    gridpack_path = pjoin(subdir, "vbfml.tgz")
    if not os.path.exists(gridpack_path) or args.overwrite:
        pack_repo(gridpack_path, os.path.abspath(subdir), overwrite=args.overwrite)

    input_files.append(gridpack_path)

    # Submission details!
    filedir = pjoin(subdir, "files")
    if not os.path.exists(filedir):
        os.makedirs(filedir)

    # Arguments to the shell script
    arguments = [
        f"--training-directory {os.path.abspath(subdir)}",
        "train",
        f"--num-epochs {args.num_epochs}",
        "--no-verbose-output",
    ]

    submission_settings = {
        "Initialdir": subdir,
        "executable": vbfml_path("execute/htcondor_wrap.sh"),
        "arguments": " ".join(arguments),
        "should_transfer_files": "YES",
        "when_to_transfer_output": "ON_EXIT",
        "transfer_input_files": ", ".join(input_files),
        "Output": pjoin(filedir, "out.txt"),
        "Error": pjoin(filedir, "err.txt"),
        "log": pjoin(filedir, "log.txt"),
        "request_cpus": str(args.jobs),
        "request_memory": str(args.memory if args.memory else 2500),
        "+MaxRuntime": f"{60*60*48}",
        # "on_exit_remove" : "((ExitBySignal == False) && (ExitCode == 0)) || (NumJobStarts >= 2)",
    }

    # Prepare the jdl job file
    sub = htcondor.Submit(submission_settings)
    jdl = pjoin(filedir, "job_file.jdl")
    with open(jdl, "w") as f:
        f.write(str(sub))
        f.write("\nqueue 1\n")

    # Go ahead with the submission
    if args.dryrun:
        print("Dry run completed.")
    else:
        jobid = condor_submit(jdl)
        print(f"Submitted job ID: {jobid}")


def main():
    args = parse_cli()
    do_submit(args)


if __name__ == "__main__":
    main()
