#!/usr/bin/env python3

import os
import argparse
import warnings
import logging
import pandas as pd
import tensorflow as tf
import torch

from datetime import datetime

from vbfml.util import (
    vbfml_path,
    write_repo_version,
    ModelConfiguration,
    ModelFactory,
)
from vbfml.helpers.deployment import pack_repo
from vbfml.helpers.condor import condor_submit
from vbfml.training.util import do_setup

pjoin = os.path.join

# Ignore pandas performance warnings for now
warnings.filterwarnings("ignore", category=pd.errors.PerformanceWarning)

# Tensorflow can be more silent
logging.getLogger("tensorflow").setLevel(logging.ERROR)


def parse_cli():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-i",
        "--input_dir",
        default=None,
        help="Path to the directory containing input ROOT files.",
    )
    parser.add_argument(
        "-m",
        "--model_config",
        default=None,
        help="Path to the model configuration yml file.",
    )
    parser.add_argument(
        "-n",
        "--num_epochs",
        type=int,
        default=10,
        help="Number of iterations through the whole training set.",
    )
    parser.add_argument(
        "-lr",
        "--learning_rate",
        type=int,
        default=0.001,
        help="Learning rate",
    )
    parser.add_argument(
        "--name", default=None, help="Job name specifying this particular submission."
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="If a submission directory exists, whether to overwrite. Defaults to False.",
    )
    parser.add_argument(
        "--jobs", type=int, default=1, help="Number of cores to request."
    )
    parser.add_argument(
        "--memory", type=int, default=None, help="Maximum amount of memory to request."
    )
    parser.add_argument("--dryrun", action="store_true", help="Dry run flag.")
    args = parser.parse_args()
    return args


def do_submit(args):
    """
    Sets up a training area with the given model configuration,
    and submits a job to HTCondor to run the training.
    """
    import htcondor

    # Create the submission directory
    if args.name:
        subdir = os.path.abspath(pjoin("./submission", args.name))
        if os.path.exists(subdir) and not args.overwrite:
            raise RuntimeError(
                f"Will not use existing submission directory unless --overwrite is specified: {subdir}"
            )
    else:
        timetag = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        subdir = os.path.abspath(pjoin("./submission", timetag))

    if not os.path.exists(subdir):
        os.makedirs(subdir)

    # Set up the training area
    do_setup(
        output_directory=subdir,
        input_dir=os.path.abspath(args.input_dir),
        model_config=args.model_config,
    )

    # Set up the model and save into the training directory
    mconfig = ModelConfiguration(args.model_config)
    model = ModelFactory.build(mconfig)

    model_arch = mconfig.get("architecture")
    assert model_arch in ["dense", "conv"], f"Unknown model architecture: {model_arch}"

    # For dense models, we will use PyTorch
    if model_arch == "dense":
        print("\nPyTorch DNN model:")
        print(f"\n{model}\n")

        def prepend_path(fname):
            return os.path.join(subdir, fname)

        # Save the model
        torch.save(model, prepend_path("model.pt"))

    # Keras models for CNNs
    else:
        optimizer = tf.keras.optimizers.Adam(
            learning_rate=0.001,
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-07,
            amsgrad=False,
            name="Adam",
        )
        model.compile(
            loss="categorical_crossentropy",
            optimizer=optimizer,
            weighted_metrics=["categorical_accuracy"],
        )
        model.summary()

        model.save(pjoin(subdir, "models/latest"), include_optimizer=True)

    # Repo version information
    write_repo_version(pjoin(subdir, "version.txt"))

    # List of input files/directories we want to ship to the execution machine
    input_files = []

    # Pack the repository for deployment to execution machine
    gridpack_path = pjoin(subdir, "vbfml.tgz")
    if not os.path.exists(gridpack_path) or args.overwrite:
        pack_repo(gridpack_path, os.path.abspath(subdir), overwrite=args.overwrite)

    input_files.append(gridpack_path)

    # Submission details!
    filedir = pjoin(subdir, "files")
    if not os.path.exists(filedir):
        os.makedirs(filedir)

    # Arguments to the shell script
    arguments = [
        f"--training-directory {os.path.abspath(subdir)}",
        "train",
        f"--num-epochs {args.num_epochs}",
        "--no-verbose-output",
        f"--learning-rate {args.learning_rate}",
    ]

    submission_settings = {
        "Initialdir": subdir,
        "executable": vbfml_path("execute/htcondor_wrap.sh"),
        "arguments": " ".join(arguments),
        "should_transfer_files": "YES",
        "when_to_transfer_output": "ON_EXIT",
        "transfer_input_files": ", ".join(input_files),
        "Output": pjoin(filedir, "out.txt"),
        "Error": pjoin(filedir, "err.txt"),
        "log": pjoin(filedir, "log.txt"),
        "request_cpus": str(args.jobs),
        "request_memory": str(args.memory if args.memory else 2500),
        "+MaxRuntime": f"{60*60*48}",
        # "on_exit_remove" : "((ExitBySignal == False) && (ExitCode == 0)) || (NumJobStarts >= 2)",
    }

    # Prepare the jdl job file
    sub = htcondor.Submit(submission_settings)
    jdl = pjoin(filedir, "job_file.jdl")
    with open(jdl, "w") as f:
        f.write(str(sub))
        f.write("\nqueue 1\n")

    # Go ahead with the submission
    if args.dryrun:
        print("Dry run completed.")
    else:
        jobid = condor_submit(jdl)
        print(f"Submitted job ID: {jobid}")


def main():
    args = parse_cli()
    do_submit(args)


if __name__ == "__main__":
    main()
