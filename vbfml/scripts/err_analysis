#!/usr/bin/env python3
import copy
import os
import re
import warnings
import click
import pickle

import tensorflow as tf
import numpy as np
import pandas as pd
from sklearn.metrics import precision_recall_curve, auc, roc_curve
from tabulate import tabulate

from tqdm import tqdm
from glob import glob
from matplotlib import pyplot as plt
from typing import Tuple

from vbfml.util import (
    DatasetAndLabelConfiguration,
    get_process_tag_from_file,
    vbfml_path,
)
from vbfml.plot.util import Quantity
from vbfml.training.util import summarize_datasets, select_and_label_datasets
from vbfml.training.data import TrainingLoader
from vbfml.training.plot import ImagePlotter
from vbfml.training.input import build_sequence, load_datasets_bucoffea

warnings.filterwarnings("ignore", category=pd.errors.PerformanceWarning)

pjoin = os.path.join


def load_cache(input_dir: str) -> tuple:
    """
    Load prediction data from cache and return it.
    """
    cache_file = pjoin(input_dir, "predict_cache.pkl")
    assert os.path.exists(cache_file), f"Cannot find file: {cache_file}"
    with open(cache_file, "rb") as f:
        cache = pickle.load(f)

    df = cache["df_non_feature"]
    predictions = cache["predictions"]
    label_encoding = cache["label_encoding"]

    return df, predictions, label_encoding


@click.group()
def cli():
    pass


@cli.command()
@click.pass_context
@click.option(
    "-i",
    "--input-files",
    required=True,
    help="Path to the directory with the input ROOT files.",
)
@click.option(
    "-m",
    "--model-path",
    required=True,
    help="Path to the model directory.",
)
@click.option(
    "-t",
    "--tag",
    required=True,
    help="Tag to identify the process.",
)
@click.option(
    "-r1",
    "--range-seq1",
    required=False,
    type = float,
    default = 0.8,
    help="read_range from ",
)
@click.option(
    "-r2",
    "--range-seq2",
    required=False,
    type = float,
    default = 1.0,
    help="read_range to",
)
@click.option(
    "--save-images",
    is_flag=True,
    help="If specified, the images will be saved as another pkl file within the directory.",
)
def predict(
    ctx, input_files: str, model_path: str, tag: str, range_seq1: int, range_seq2: int , save_images: bool = False
) -> None:
    """
    Read events from the input_files, make predictions
    with the pre-trained model (read from model_path) and save the
    predictions, together with other event data for later use.

    input_files can be either a single file, or can contain an asterisk (*)
    to specify multiple files.
    """

    # Create a list of DatasetInfo objects for the files that we are interested in
    directory, file_pattern = os.path.dirname(input_files), os.path.basename(
        input_files
    )
    datasets = load_datasets_bucoffea(directory, file_pattern)
    # Get datasets and corresponding labels from datasets.yml
    datasets_path = vbfml_path("config/datasets/datasets.yml")
    dataset_config = DatasetAndLabelConfiguration(datasets_path)

    dataset_labels = dataset_config.get_dataset_labels()
    datasets = select_and_label_datasets(datasets, dataset_labels)
    summarize_datasets(datasets)

    high_level_features = [
        "mjj",
        "detajj",
        "njet",
        "njet_pt30",
        "njet_central",
        "njet_forward",
        "recoil_pt",
        "recoil_phi",
        "leadak4_pt",
        "trailak4_pt",
        "leadak4_eta",
        "trailak4_eta",
        "leadak4_phi",
        "trailak4_phi",
        "ak4_pt2",
        "ak4_eta2",
        "ak4_phi2",
    ]

    # read feature from pkl file
    with open(model_path + "/features.pkl", "rb") as f:
        image_features = pickle.load(f)

    # Validation sequence: Read the last 20% of events
    read_range = (range_seq1, range_seq2)

    # Two separate validation sequences for high_level features (e.g. mjj) and image features:
    # We'll make the predictions based on the image features, but we'll plot the
    # high-level quantities at the end
    validation_sequences = {}

    validation_sequences["high_level"] = build_sequence(
        datasets=copy.deepcopy(datasets),
        features=high_level_features,
        weight_expression="weight_total*xs/sumw",
        shuffle=True,
        scale_features="none",
    )
    validation_sequences["high_level"].batch_size = int(1e6)
    validation_sequences["high_level"].batch_buffer_size = 1

    validation_sequences["image"] = build_sequence(
        datasets=copy.deepcopy(datasets),
        features=image_features,
        weight_expression="weight_total*xs/sumw",
        shuffle=True,
        scale_features="norm",
    )

    validation_sequences["image"].batch_size = int(1e3)
    validation_sequences["image"].batch_buffer_size = 10

    for sequence in validation_sequences.values():
        sequence.read_range = read_range

    # Load the pre-trained model
    loader = TrainingLoader(model_path)
    model = loader.get_model()

    # Retrieve the label encoding from the original training sequence
    # so that we have the correct integer to label mapping
    training_sequence = loader.get_sequence("training")
    temp = training_sequence.label_encoding
    # Clean the non-int keys
    label_encoding = {k: v for k, v in temp.items() if isinstance(k, int)}

    predictions = []
    image_pixels = []
    for ibatch in tqdm(
        range(len(validation_sequences["image"])), desc="Making predictions"
    ):
        features, _, _ = validation_sequences["image"][ibatch]
        predictions.append(model.predict(features)[:,1])
        image_pixels.append(features)

    predictions = np.concatenate(predictions)
    image_pixels = np.concatenate(image_pixels)

    # High-level features
    for ibatch in tqdm(
        range(len(validation_sequences["high_level"])),
        desc="Obtaining high-level features",
    ):
        # Fill the sequence buffer if the batch is not there
        validation_sequences["high_level"][ibatch]
        df_non_feature = validation_sequences["high_level"].buffer.get_batch_df(ibatch)
        df_non_feature.drop(columns=["label"], inplace=True)

    outdir = pjoin(model_path, f"predictions_{tag.lower()}")
    if not os.path.exists(outdir):
        os.makedirs(outdir)

    # Dump the input file argument to a txt file
    input_list_file = pjoin(outdir, "input_root_files.txt")
    with open(input_list_file, "w+") as f:
        for infile in glob(input_files):
            f.write(f"{infile}\n")

    print(len(predictions))
    print(len(df_non_feature))
    # Save everything into a cache.pkl file
    cache = {
        "predictions": predictions,
        "df_non_feature": df_non_feature,
        "label_encoding": label_encoding,
    }

    cache_file = pjoin(outdir, "predict_cache.pkl")
    with open(cache_file, "wb+") as f:
        pickle.dump(cache, f)

    # Also save the image arrays for later use, if requested specifically
    if save_images:
        images_cache = pjoin(outdir, "images.pkl")
        with open(images_cache, "wb+") as f:
            pickle.dump(image_pixels, f)


@cli.command()
@click.pass_context
@click.argument("input_dir")
@click.option("-n", "--normalize", is_flag=True, help="Normalize the histogram plots.")
def plot(ctx, input_dir: str, normalize: bool) -> None:
    """
    Make histogram of high-level features, split by the predicted class,
    and plot the histograms.
    """
    df, predictions, label_encoding = load_cache(input_dir)

    process_tag = os.path.basename(input_dir.rstrip("/")).replace("predictions_", "")

    outdir = pjoin(input_dir, "plots")
    if not os.path.exists(outdir):
        os.makedirs(outdir)

    # Based on the predictions, we make histograms for different classes
    quantities = [x for x in list(df.columns) if x != "weight"]

    for quantity_name in tqdm(quantities, desc="Plotting histograms"):
        quantity = Quantity(quantity_name)
        fig, ax = plt.subplots()
        for icls, sample_cls in label_encoding.items():
            mask = predictions.round() == icls
            try:
                ax.hist(
                    df[quantity_name][mask],
                    histtype="step",
                    weights=df["weight"][mask],
                    bins=quantity.bins,
                    label=sample_cls,
                    density=normalize,
                )
            except KeyError:
                print(f"WARNING: Cannot find {quantity_name} in dataframe, skipping.")
                continue

        ax.set_xlabel(quantity.label, fontsize=14)
        if normalize:
            ax.set_ylabel("Weighted Counts (Norm.)", fontsize=14)
        else:
            ax.set_ylabel("Weighted Counts", fontsize=14)

        ax.set_yscale("log")

        ax.legend(title="Predicted Class")

        ax.text(
            1,
            1,
            f"# Events: {len(predictions)}",
            fontsize=14,
            ha="right",
            va="bottom",
            transform=ax.transAxes,
        )
        ax.text(
            0,
            1,
            process_tag,
            fontsize=14,
            ha="left",
            va="bottom",
            transform=ax.transAxes,
        )

        outpath = pjoin(outdir, f"{quantity_name}.pdf")
        fig.savefig(outpath)
        plt.close(fig)


@cli.command()
@click.pass_context
@click.argument("input_dir")
@click.option(
    "-q", "--quantity", required=True, help="The quantity to compute the threshold."
)
@click.option(
    "-t", "--threshold", type=float, required=True, help="The threshold value."
)
def average(ctx, input_dir: str, quantity: str, threshold: float) -> None:
    """
    Given the set of predictions and the high-level features (e.g. mjj),
    compute the average image per class (QCD V / EWK V) for events that
    satisfy:

    quantity > threshold.
    """
    # Gather the data
    df, predictions, label_encoding = load_cache(input_dir)

    with open(pjoin(input_dir, "input_root_files.txt"), "r") as f:
        inputfile = f.readlines()[0]

    process_tag = get_process_tag_from_file(inputfile)

    outdir = pjoin(input_dir, "averaged")
    if not os.path.exists(outdir):
        os.makedirs(outdir)

    plotter = ImagePlotter()

    # Load the images
    with open(pjoin(input_dir, "images.pkl"), "rb") as f:
        image_pixels = pickle.load(f)

    # Get the images for the events where quantity > threshold
    # and compute the average for these images.
    mask = df[quantity] > threshold
    classes = ["bkg", "signal"]
    for iclass, class_label in enumerate(
        tqdm(classes, desc="Plotting averaged images")
    ):
        image_mask = mask & (predictions == iclass)
        images = image_pixels[image_mask]
        weights = df["weight"][image_mask]

        avg_image = np.average(images, axis=0, weights=weights)
        # outdir = pjoin(input_dir, "plots")

        plotter.plot(
            image=avg_image,
            outdir=outdir,
            filename=f"{class_label}_{quantity}_gt_{threshold}.pdf",
            vmin=1e-3,
            vmax=6e-2,
            cbar_label="Average Energy (GeV)",
            left_label=process_tag,
            right_label="predict : "
            + class_label
            + f"({sum(image_mask)}/{len(image_pixels)})",
        )


@cli.command()
@click.pass_context
@click.argument("input_dir")
@click.option(
    "-s",
    "--sequence-type",
    required=False,
    default="validation",
    help="The type of sequence: training or validation.",
)
def evaluate(ctx, input_dir: str, sequence_type: str) -> None:
    """
    Evalute the accuracy of the pre-trained model on full sequence.
    """
    loader = TrainingLoader(input_dir)

    model = loader.get_model()
    sequence = loader.get_sequence(sequence_type)

    sequence.batch_size = int(1e3)
    sequence.batch_buffer_size = 100

    model.evaluate(sequence)

@cli.command()
@click.pass_context
@click.option(
    "-i",
    "--input-dir",
    required=True,
    multiple=True,
    help= "files to compare")
@click.option(
    "-n",
    "--number-epochs",
    required=False,
    type = int,
    default=10,
    help="The number of last epochs considered in the statistics",
)
@click.option(
    "-s",
    "--save_name",
    required=False,
    type = str,
    default = None,
    help="If specified, clarify the name of the pdf save file",
)
def compare_hist(ctx, input_dir: str, number_epochs: int, save_name: str) -> None:
    """
    Compare the statistics of the history of different models.
    """

    for dir in input_dir :
        cache_file = pjoin(dir, "history.pkl")
        assert os.path.exists(cache_file), f"Cannot find file: {cache_file}"
        with open(cache_file, "rb") as f:
            hist = pickle.load(f)

        x = hist['x_val_categorical_accuracy'][-number_epochs :]
        acc  = hist['y_val_categorical_accuracy'][-number_epochs :]
        loss_tr  = hist['y_loss'][-number_epochs :]
        loss_val  = hist['y_val_loss'][-number_epochs :]
        # stat = f"[{min(acc)*100:.2f}% <- {sum(acc)/len(acc)*100:.2f}% -> {max(acc)*100:.2f}%]"

        print("\n statistics of model " + os.path.basename(dir) + 
             f"\n [min <- mean -> max] over last {number_epochs} epochs" + 
             f"\n accuracy : [{min(acc)*100:.2f}% <- {sum(acc)/len(acc)*100:.2f}% -> {max(acc)*100:.2f}%]" +
             f"\n standard deviation : {np.std(acc)*100:.2f}%" + 
             f"\n ended with acc : {acc[number_epochs-1]*100:.2f}%\n")
        
        plt.figure(1)
        plt.plot(x,acc,'.-', markersize=15, label=os.path.basename(dir))
        plt.figure(2)
        plt.plot(x,loss_tr,'.-', markersize=15, label=os.path.basename(dir))
        plt.figure(3)
        plt.plot(x,loss_val,'.-', markersize=15, label=os.path.basename(dir))

    outdir = pjoin(os.path.dirname(input_dir[0]), "comparison"+save_name)
    if not os.path.exists(outdir):
        os.makedirs(outdir)

    plt.figure(1)
    plt.legend()
    plt.title("comparison validation accuracy")
    plt.xlabel("epochs")
    plt.ylabel("accuracy")
    plt.savefig(outdir+"/comparison_val_acc"+save_name+".pdf")

    plt.figure(2)
    plt.legend()
    plt.title("comparison training loss")
    plt.xlabel("epochs")
    plt.ylabel("Loss")
    plt.savefig(outdir+"/comparison_train_loss"+save_name+".pdf")

    plt.figure(3)
    plt.legend()
    plt.title("comparison validation loss ")
    plt.xlabel("epochs")
    plt.ylabel("Loss")
    plt.savefig(outdir+"/comparison_val_loss"+save_name+".pdf")

@cli.command()
@click.pass_context
@click.argument("input_dir")
def roc(ctx, input_dir: str) -> None:
    """
    compute the roc curve of the model.
    """

    #load model and validation sequence
    loader = TrainingLoader(input_dir)
    model = loader.get_model()
    validation_sequence = loader.get_sequence("validation")

    #load predicted data
    for i in range(validation_sequence.__len__()) :
        print(f"batch [{i+1}/{validation_sequence.__len__()}]")
        _, labels_onehot, _ = validation_sequence[i]
        labels_batch = labels_onehot.argmax(axis=1)
        if i == 0 : labels = labels_batch
        else : labels = np.concatenate((labels,labels_batch))

    # make prediction
    print("making the prediction with the model")
    predict = model.predict(validation_sequence)[:,1]

    # compute ROC and precision recall curve
    print("compute precision recall")
    precision, recall, thresholds_prc = precision_recall_curve(labels, predict)

    print("compute ROC")
    fpr, tpr, thresholds_roc = roc_curve(labels, predict)

    # write data into pickle file
    print("write data into pickle file")
    data = {"fpr": fpr, "tpr": tpr, "thresh_roc": thresholds_roc,
            "precision": precision, "recall": recall, "thresholds_prc": thresholds_prc}

    with open(input_dir+"/roc.pkl", 'wb') as f:
        pickle.dump(data,f)

@cli.command()
@click.pass_context
@click.option(
    "-i",
    "--input-dir",
    required=True,
    multiple=True,
    help= "files to compare")
@click.option(
    "-s",
    "--save_name",
    required=False,
    type = str,
    default = None,
    help="If specified, clarify the name of the pdf save file",
)
def plot_roc(ctx, input_dir: str, save_name: str) -> None:
    """
    plot the roc curve of the model.
    """

    for dir in input_dir :
        cache_file = pjoin(dir, "roc.pkl")
        assert os.path.exists(cache_file), f"Cannot find file: {cache_file}"

        #read data
        with open(cache_file, "rb") as f:
            data = pickle.load(f)

        # compute area under the curve (AUC)
        auc_roc = auc(data['fpr'],data['tpr'])
        auc_prc = auc(data['recall'],data['precision'])

        # get the index of a 0.5 cut
        middle_thres_index = np.where(data['thresh_roc'] < 0.5)[0][0]
        
        # plot graphs
        plt.figure(1)
        plt.plot(data['fpr'],data['tpr'], label= os.path.basename(dir) + f'(AUC = {auc_roc:.3f})')

        plt.figure(2)
        plt.plot(data['recall'],data['precision'], label= os.path.basename(dir) + f'(AUC = {auc_prc:.3f})')

    if len(input_dir) == 1 : 
        outdir  = input_dir[0]
    else : 
        outdir = pjoin(os.path.dirname(input_dir[0]), "comparison"+save_name)
    if not os.path.exists(outdir):
        os.makedirs(outdir)
    
    # complete graphs
    plt.figure(1)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.title('ROC curve') 
    plt.xlabel('False positive rate')
    plt.ylabel('True positive rate')
    plt.legend()
    plt.savefig(outdir+"/roc_curve.pdf")

    plt.figure(2)
    plt.title('Precision recall curve')
    plt.xlabel('recall')
    plt.ylabel('precision')
    plt.legend()
    plt.savefig(outdir+"/prc_curve.pdf")

@cli.command()
@click.pass_context
@click.option(
    "-i",
    "--input-dir",
    required=True,
    help= "files to compare")
def plot_correl(ctx, input_dir: str) -> None:
    """
    plot the correlation plot for all variable available according to the score.
    """

    #create outdir
    outdir = pjoin(input_dir, "correlation_scatter")
    if not os.path.exists(outdir):
        os.makedirs(outdir)

    #load data
    features, predictions, label_encoding = load_cache(input_dir)

    print(f"number of event {len(predictions)}")

    # prepare variables for table
    correl = []

    markersize = 100 / np.sqrt(len(predictions))
    count=1
    # plot correlation plots
    for (featurename, feature) in features.iteritems():

        print("plot "+featurename+f"({count}/{len(features.keys())})")

        # take only the positive value of eta to find a linear correlation
        if (featurename == "leadak4_eta") or (featurename == "trailak4_eta"): 
            feature = abs(feature)
            featurename = "abs_val_" + featurename

        correlation = np.corrcoef(predictions, feature)[0,1]
        correl.append([correlation,featurename])

        plt.figure()
        plt.scatter(predictions, feature, s=markersize, label= f"correlation = {correlation:.3f}")
        plt.title('score correlation with ' +  featurename) 
        plt.xlabel('Score')
        plt.ylabel(featurename)
        plt.legend()
        plt.savefig(outdir+"/correl_"+featurename+".pdf")
        count += 1
    
    def absfirst(a :list):
        return abs(a[0])
    
    correl.sort(key=absfirst,reverse=True)
    correl = np.array(correl)

    table = {"variables" : correl[:,1], "score_correlation" : correl[:,0]}
    table_print = tabulate(table, headers='keys')
    print(table_print)

    with open(outdir+'/table.txt', 'w') as f:
        f.write(table_print)

    

if __name__ == "__main__":
    cli()
