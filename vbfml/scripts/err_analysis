#!/usr/bin/env python3
import copy
import os
import re
import warnings
import click
import pickle

import tensorflow as tf
import numpy as np
import pandas as pd
from sklearn.metrics import precision_recall_curve, auc, roc_curve
from tabulate import tabulate

from tqdm import tqdm
from glob import glob
from matplotlib import pyplot as plt
from typing import Tuple

from vbfml.util import (
    DatasetAndLabelConfiguration,
    get_process_tag_from_file,
    vbfml_path,
)
from vbfml.plot.util import (
    Quantity,
    plot_histograms_for_each_label,
    sort_by_pair_along_first_axis,
)
from vbfml.training.util import summarize_datasets, select_and_label_datasets
from vbfml.training.data import TrainingLoader
from vbfml.training.plot import ImagePlotter
from vbfml.training.input import build_sequence, load_datasets_bucoffea

warnings.filterwarnings("ignore", category=pd.errors.PerformanceWarning)

pjoin = os.path.join


def load_cache(input_dir: str) -> tuple:
    """
    Load prediction data from cache and return it.
    """
    cache_file = pjoin(input_dir, "predict_cache.pkl")
    assert os.path.exists(cache_file), f"Cannot find file: {cache_file}"
    with open(cache_file, "rb") as f:
        cache = pickle.load(f)

    df = cache["df_non_feature"]
    predictions = cache["predictions"]
    label_encoding = cache["label_encoding"]
    labels = cache["labels"]

    return df, predictions, label_encoding, labels


@click.group()
def cli():
    pass


@cli.command()
@click.pass_context
@click.option(
    "-i",
    "--input-files",
    required=True,
    help="Path to the directory with the input ROOT files.",
)
@click.option(
    "-m",
    "--model-path",
    required=True,
    help="Path to the model directory.",
)
@click.option(
    "-t",
    "--tag",
    required=True,
    help="Tag to identify the process.",
)
@click.option(
    "-r1",
    "--range-seq1",
    required=False,
    type=float,
    default=0.8,
    help="read_range from ... (adjust read_range in the sequence to allow to read from any event)",
)
@click.option(
    "-r2",
    "--range-seq2",
    required=False,
    type=float,
    default=1.0,
    help="read_range to ... (adjust read_range in the sequence to allow to read to any event)",
)
@click.option(
    "--save-images",
    is_flag=True,
    help="If specified, the images will be saved as another pkl file within the directory.",
)
def predict(
    ctx,
    input_files: str,
    model_path: str,
    tag: str,
    range_seq1: int,
    range_seq2: int,
    save_images: bool = False,
) -> None:
    """
    Read events from the input_files, make predictions
    with the pre-trained model (read from model_path) and save the
    predictions, together with other event data for later use.

    input_files can be either a single file, or can contain an asterisk (*)
    to specify multiple files.
    for a specfic selection, brackets ([]) can be used to select specific range of character
    for instance "tree_[E-V]*" will select EWK and VBF,
    or "tree_[V-Z]*" will select VBF and all QCD (W, Z1, Z2)
    """

    # Create a list of DatasetInfo objects for the files that we are interested in
    directory = os.path.dirname(input_files)
    file_pattern = os.path.basename(input_files)

    datasets = load_datasets_bucoffea(directory, file_pattern)

    # Get datasets and corresponding labels from datasets.yml
    datasets_path = vbfml_path("config/datasets/datasets.yml")
    dataset_config = DatasetAndLabelConfiguration(datasets_path)

    dataset_labels = dataset_config.get_dataset_labels()
    datasets = select_and_label_datasets(datasets, dataset_labels)
    summarize_datasets(datasets)

    high_level_features = [
        "mjj",
        "detajj",
        "dphijj",
        "njet",
        "njet_pt30",
        "njet_central",
        "njet_forward",
        "recoil_pt",
        "recoil_phi",
        "leadak4_pt",
        "trailak4_pt",
        "leadak4_eta",
        "trailak4_eta",
        "leadak4_phi",
        "trailak4_phi",
        "ak4_pt2",
        "ak4_eta2",
        "ak4_phi2",
    ]

    # read feature from pkl file
    with open(f"{model_path}/features.pkl", "rb") as f:
        image_features = pickle.load(f)

    # Get the read range from the command line, default is (0.8, 1.0)
    read_range = (range_seq1, range_seq2)

    # Two separate validation sequences for high_level features (e.g. mjj) and image features:
    # We'll make the predictions based on the image features, but we'll plot the
    # high-level quantities at the end
    validation_sequences = {}

    validation_sequences["high_level"] = build_sequence(
        datasets=copy.deepcopy(datasets),
        features=high_level_features,
        weight_expression="weight_total*xs/sumw",
        shuffle=True,
        scale_features="none",
    )
    validation_sequences["high_level"].batch_size = int(1e3)
    validation_sequences["high_level"].batch_buffer_size = 10

    validation_sequences["image"] = build_sequence(
        datasets=copy.deepcopy(datasets),
        features=image_features,
        weight_expression="weight_total*xs/sumw",
        shuffle=True,
        scale_features="norm",
    )

    validation_sequences["image"].batch_size = int(1e3)
    validation_sequences["image"].batch_buffer_size = 10

    for sequence in validation_sequences.values():
        sequence.read_range = read_range

    # Load the pre-trained model
    loader = TrainingLoader(model_path)
    model = loader.get_model()

    # Retrieve the label encoding from the original training sequence
    # so that we have the correct integer to label mapping
    training_sequence = loader.get_sequence("training")
    temp = training_sequence.label_encoding
    # Clean the non-int keys
    label_encoding = {k: v for k, v in temp.items() if isinstance(k, int)}

    predictions = []
    image_pixels = []
    labels = []
    for ibatch in tqdm(
        range(len(validation_sequences["image"])), desc="Making predictions"
    ):
        features, labels_onehot, _ = validation_sequences["image"][ibatch]

        # only takes the score value for the predictions with [:, 1]
        # ex : model.predict [0.26; 0.74] -> predictions 0.74
        predictions.append(model.predict(features)[:, 1])
        image_pixels.append(features)
        labels.append(labels_onehot.argmax(axis=1))

    predictions = np.concatenate(predictions)
    image_pixels = np.concatenate(image_pixels)
    labels = np.concatenate(labels)

    # High-level features
    high_level_dfs = []
    for ibatch in tqdm(
        range(len(validation_sequences["high_level"])),
        desc="Obtaining high-level features",
    ):
        # Fill the sequence buffer if the batch is not there
        if ibatch not in validation_sequences["high_level"].buffer:
            validation_sequences["high_level"][ibatch]
        high_level_df = validation_sequences["high_level"].buffer.get_batch_df(ibatch)
        high_level_df.drop(columns=["label"], inplace=True)

        high_level_dfs.append(high_level_df)

    df_non_feature = pd.concat(high_level_dfs, ignore_index=True)

    assert len(predictions) == len(
        df_non_feature
    ), f"Length of predictions and df_non_feature are not matching ({len(predictions)}-{len(df_non_feature)})"

    outdir = pjoin(model_path, f"predictions_{tag.lower()}")
    if not os.path.exists(outdir):
        os.makedirs(outdir)

    # Dump the input file argument to a txt file
    input_list_file = pjoin(outdir, "input_root_files.txt")
    with open(input_list_file, "w+") as f:
        for infile in glob(input_files):
            f.write(f"{infile}\n")

    # Save everything into a cache.pkl file
    cache = {
        "predictions": predictions,
        "df_non_feature": df_non_feature,
        "label_encoding": label_encoding,
        "labels": labels,
    }

    cache_file = pjoin(outdir, "predict_cache.pkl")
    with open(cache_file, "wb+") as f:
        pickle.dump(cache, f)

    # Also save the image arrays for later use, if requested specifically
    if save_images:
        images_cache = pjoin(outdir, "images.pkl")
        with open(images_cache, "wb+") as f:
            pickle.dump(image_pixels, f)


@cli.command()
@click.pass_context
@click.argument("input_dir")
@click.option("-n", "--normalize", is_flag=True, help="Normalize the histogram plots.")
def plot(ctx, input_dir: str, normalize: bool) -> None:
    """
    Make histogram of high-level features, split by the predicted class,
    and plot the histograms.
    """
    df, predictions, label_encoding, _ = load_cache(input_dir)

    process_tag = os.path.basename(input_dir.rstrip("/")).replace("predictions_", "")

    outdir = pjoin(input_dir, "plots")
    if not os.path.exists(outdir):
        os.makedirs(outdir)

    # Based on the predictions, we make histograms for different classes
    quantities = [x for x in list(df.columns) if x not in ["weight", "dataset_label"]]

    for quantity_name in tqdm(quantities, desc="Plotting histograms"):
        quantity = Quantity(quantity_name)
        fig, ax = plt.subplots()
        for icls, sample_cls in label_encoding.items():
            mask = predictions.round() == icls
            try:
                ax.hist(
                    df[quantity_name][mask],
                    histtype="step",
                    weights=df["weight"][mask],
                    bins=quantity.bins,
                    label=sample_cls,
                    density=normalize,
                )
            except KeyError:
                print(f"WARNING: Cannot find {quantity_name} in dataframe, skipping.")
                continue

        ax.set_xlabel(quantity.label, fontsize=14)
        if normalize:
            ax.set_ylabel("Weighted Counts (Norm.)", fontsize=14)
        else:
            ax.set_ylabel("Weighted Counts", fontsize=14)

        ax.set_yscale("log")

        ax.legend(title="Predicted Class")

        ax.text(
            1,
            1,
            f"# Events: {len(predictions)}",
            fontsize=14,
            ha="right",
            va="bottom",
            transform=ax.transAxes,
        )
        ax.text(
            0,
            1,
            process_tag,
            fontsize=14,
            ha="left",
            va="bottom",
            transform=ax.transAxes,
        )

        outpath = pjoin(outdir, f"{quantity_name}.pdf")
        fig.savefig(outpath)
        plt.close(fig)


@cli.command()
@click.pass_context
@click.argument("input_dir")
@click.option(
    "-q", "--quantity", required=True, help="The quantity to compute the threshold."
)
@click.option(
    "-t", "--threshold", type=float, required=True, help="The threshold value."
)
def average(ctx, input_dir: str, quantity: str, threshold: float) -> None:
    """
    Given the set of predictions and the high-level features (e.g. mjj),
    compute the average image per class (QCD V / EWK V) for events that
    satisfy:

    quantity > threshold.
    """
    # Gather the data
    df, predictions, label_encoding, _ = load_cache(input_dir)

    with open(pjoin(input_dir, "input_root_files.txt"), "r") as f:
        inputfile = f.readlines()[0]

    process_tag = get_process_tag_from_file(inputfile)

    outdir = pjoin(input_dir, "averaged")
    if not os.path.exists(outdir):
        os.makedirs(outdir)

    plotter = ImagePlotter()

    # Load the images
    with open(pjoin(input_dir, "images.pkl"), "rb") as f:
        image_pixels = pickle.load(f)

    # Get the images for the events where quantity > threshold
    # and compute the average for these images.
    mask = df[quantity] > threshold
    classes = ["bkg_17", "signal_17"]
    for iclass, class_label in enumerate(
        tqdm(classes, desc="Plotting averaged images")
    ):
        image_mask = mask & (predictions.round() == iclass)
        images = image_pixels[image_mask]
        weights = df["weight"][image_mask]

        avg_image = np.average(images, axis=0, weights=weights)

        plotter.plot(
            image=avg_image,
            outdir=outdir,
            filename=f"{class_label}_{quantity}_gt_{threshold}.pdf",
            vmin=1e-3,
            vmax=6e-2,
            cbar_label="Average Energy (GeV)",
            left_label=process_tag,
            right_label=f"predict :{class_label}({sum(image_mask)}/{sum(mask)})",
        )


@cli.command()
@click.pass_context
@click.argument("input_dir")
@click.option(
    "-s",
    "--sequence-type",
    required=False,
    default="validation",
    help="The type of sequence: training or validation.",
)
def evaluate(ctx, input_dir: str, sequence_type: str) -> None:
    """
    Evalute the accuracy of the pre-trained model on full sequence.
    """
    loader = TrainingLoader(input_dir)

    model = loader.get_model()
    sequence = loader.get_sequence(sequence_type)

    sequence.batch_size = int(1e3)
    sequence.batch_buffer_size = 100

    model.evaluate(sequence)


@cli.command()
@click.pass_context
@click.option(
    "-i",
    "--input-dirs",
    required=True,
    multiple=True,
    help="Training directories to compare.",
)
@click.option(
    "--is-parent",
    is_flag=True,
    help="If flag, -i take a dir of directories and compare all of them",
)
@click.option(
    "-n",
    "--number-epochs",
    required=False,
    type=int,
    default=10,
    help="The number of last epochs considered in the statistics",
)
@click.option(
    "-s",
    "--save_name",
    required=False,
    type=str,
    default="history",
    help="If specified, clarify the name of the pdf save file",
)
def compare_hist(
    ctx, input_dirs: str, number_epochs: int, save_name: str, is_parent: bool = False
) -> None:
    """
    Compare the loss and accuracy of different models.
    """
    if is_parent:
        assert (
            len(input_dirs) == 1
        ), f"Too many '-i' arguments are specified, if '--is-parent' is flaged, only one '-i' is accepted (specifying the parent directory)"
        input_dirs = glob(pjoin(input_dirs[0], f"*"))

    for dir in input_dirs:
        # Check if dir is a valid directory that contains history.pkl file
        if not os.path.isdir(dir) or not os.path.exists(pjoin(dir, "history.pkl")):
            continue

        cache_file = pjoin(dir, "history.pkl")
        with open(cache_file, "rb") as f:
            hist = pickle.load(f)

        x = hist["x_val_categorical_accuracy"][-number_epochs:]
        acc = hist["y_val_categorical_accuracy"][-number_epochs:]
        loss_tr = hist["y_loss"][-number_epochs:]
        loss_val = hist["y_val_loss"][-number_epochs:]

        plt.figure(1)
        plt.plot(x, acc, ".-", markersize=15, label=os.path.basename(dir))
        plt.figure(2)
        plt.plot(x, loss_tr, ".-", markersize=15, label=os.path.basename(dir))
        plt.figure(3)
        plt.plot(x, loss_val, ".-", markersize=15, label=os.path.basename(dir))

    outdir = pjoin(os.path.dirname(input_dirs[0]), f"comparison_{save_name}")
    if not os.path.exists(outdir):
        os.makedirs(outdir)

    plt.figure(1)
    plt.legend()
    plt.title("comparison validation accuracy")
    plt.xlabel("epochs")
    plt.ylabel("accuracy")
    plt.savefig(pjoin(outdir, f"comparison_val_acc_{save_name}.pdf"))

    plt.figure(2)
    plt.legend()
    plt.title("comparison training loss")
    plt.xlabel("epochs")
    plt.ylabel("Loss")
    plt.savefig(pjoin(outdir, f"comparison_train_loss_{save_name}.pdf"))

    plt.figure(3)
    plt.legend()
    plt.title("comparison validation loss ")
    plt.xlabel("epochs")
    plt.ylabel("Loss")
    plt.savefig(pjoin(outdir, f"comparison_val_loss_{save_name}.pdf"))


@cli.command()
@click.pass_context
@click.option(
    "-i",
    "--input-dir",
    required=True,
    help="path to the prediction directory of a model",
)
def roc(ctx, input_dir: str) -> None:
    """
    compute the roc curve of the model.
    only compute and write in pkl file so it allows to use plot-roc on several model to compare in in one plot
    """

    # load model and prediction pickle file
    loader = TrainingLoader(os.path.dirname(input_dir.rstrip("/")))
    model = loader.get_model()
    features, predictions, label_encoding, labels = load_cache(input_dir)
    mjj = features["mjj"]
    detajj = features["detajj"]
    weights = features["weight"]

    # compute ROC and precision recall curve
    fpr, tpr, thresholds_roc = roc_curve(labels, predictions, sample_weight=weights)
    precision, recall, thresholds_prc = precision_recall_curve(
        labels, predictions, sample_weight=weights
    )

    # add a dummy extra value above 1 to have a list of the same length
    thresholds_prc = np.append(thresholds_prc, 2)

    # do the same for a simple cut on mjj and detajj
    mjj_norm = mjj / mjj.max()
    detajj_norm = detajj / detajj.max()
    fpr_mjj, tpr_mjj, thresh_roc_mjj = roc_curve(
        labels, mjj_norm, sample_weight=weights
    )
    fpr_detajj, tpr_detajj, thresh_roc_detajj = roc_curve(
        labels, detajj_norm, sample_weight=weights
    )

    # sort the fpr values and their according tpr in order to use "auc" function of sklearn
    # the negative weights make the "roc_curve" not monotonic
    # the cons are that the threshold is not monotonic (but still associated to the good fpr-tpr value)
    fpr, tpr, thresholds_roc = sort_by_pair_along_first_axis(fpr, tpr, thresholds_roc)
    recall, precision, thresholds_prc = sort_by_pair_along_first_axis(
        recall, precision, thresholds_prc, reverse=True
    )
    fpr_mjj, tpr_mjj, thresh_roc_mjj = sort_by_pair_along_first_axis(
        fpr_mjj, tpr_mjj, thresh_roc_mjj
    )
    fpr_detajj, tpr_detajj, thresh_roc_detajj = sort_by_pair_along_first_axis(
        fpr_detajj, tpr_detajj, thresh_roc_detajj
    )

    output_pkl = pjoin(input_dir, "roc.pkl")
    # Mention the output file path in the logging
    print(f"Writing output to: {output_pkl}")

    data = {
        "fpr": fpr,
        "tpr": tpr,
        "thresh_roc": thresholds_roc,
        "precision": precision,
        "recall": recall,
        "thresholds_prc": thresholds_prc,
        "fpr_mjj": fpr_mjj,
        "tpr_mjj": tpr_mjj,
        "mjj_max": mjj.max(),
        "thresh_roc_mjj": thresh_roc_mjj,
        "fpr_detajj": fpr_detajj,
        "tpr_detajj": tpr_detajj,
        "thresh_roc_detajj": thresh_roc_detajj,
        "detajj_max": detajj.max(),
    }

    with open(output_pkl, "wb") as f:
        pickle.dump(data, f)


@cli.command()
@click.pass_context
@click.option(
    "-i",
    "--input-dirs",
    required=True,
    multiple=True,
    help="Path to the directory containing roc.pkl file for a given model. Can be a single path for a single model, or multiple paths for multiple models.",
)
@click.option(
    "-s",
    "--save_name",
    required=False,
    type=str,
    default="roc",
    help="If specified, clarify the name of the pdf save file",
)
@click.option(
    "--simple-cut",
    is_flag=True,
    help="If specified, plot the roc curve of simple cuts on mjj and detajj (to use with only one input-dir for the plot to be clean)",
)
def plot_roc(ctx, input_dirs: str, save_name: str, simple_cut: bool = False) -> None:
    """
    plot the roc curve of a model (and AUC).
    Can point to several models to plot all of them in one plot and compare.
    Can also compare the model roc curve with a simple cut on mjj and detajj.
    """

    for dir in input_dirs:
        cache_file = pjoin(dir, "roc.pkl")
        assert os.path.exists(cache_file), f"Cannot find file: {cache_file}"

        # read data
        with open(cache_file, "rb") as f:
            data = pickle.load(f)

        # compute area under the curve (AUC)
        auc_roc = auc(data["fpr"], data["tpr"])
        auc_prc = auc(data["recall"], data["precision"])
        if simple_cut:
            auc_roc_mjj = auc(data["fpr_mjj"], data["tpr_mjj"])
            auc_roc_detajj = auc(data["fpr_detajj"], data["tpr_detajj"])

        # get the index of a 0.5 cut
        middle_thres_roc_index = np.where(data["thresh_roc"] < 0.5)[0][0]
        middle_thres_prc_index = np.where(data["thresholds_prc"] > 0.5)[0][0]

        if simple_cut:
            # get the associated cut on mjj or detajj at this
            fpr_middle_thres_roc = data["fpr"][middle_thres_roc_index]
            cut_middle_index_mjj = np.where(data["fpr_mjj"] > fpr_middle_thres_roc)[0][
                0
            ]
            cut_middle_index_detajj = np.where(
                data["fpr_detajj"] > fpr_middle_thres_roc
            )[0][0]

            # find the associated mjj cut
            mjj_cut = data["thresh_roc_mjj"][cut_middle_index_mjj] * data["mjj_max"]
            detajj_cut = (
                data["thresh_roc_detajj"][cut_middle_index_detajj] * data["detajj_max"]
            )

        label_model = os.path.basename(os.path.dirname(dir.rstrip("/")))

        # plot graphs
        plt.figure(1)
        plt.plot(data["fpr"], data["tpr"], label=f"{label_model} (AUC = {auc_roc:.3f})")
        plt.plot(
            data["fpr"][middle_thres_roc_index],
            data["tpr"][middle_thres_roc_index],
            "ko",
        )

        plt.figure(2)
        plt.plot(
            data["recall"],
            data["precision"],
            label=f"{label_model} (AUC = {auc_prc:.3f})",
        )
        plt.plot(
            data["recall"][middle_thres_prc_index],
            data["precision"][middle_thres_prc_index],
            "ko",
        )

    if len(input_dirs) == 1:
        outdir = pjoin(input_dirs[0], f"plots_{save_name}")
    else:
        outdir = pjoin(
            os.path.dirname(os.path.dirname(input_dirs[0].rstrip("/"))),
            f"comparison_{save_name}",
        )
    if not os.path.exists(outdir):
        os.makedirs(outdir)

    # complete graphs
    if simple_cut:
        plt.figure(1)
        plt.plot(
            data["fpr_mjj"],
            data["tpr_mjj"],
            "m-.",
            label=f"mjj cut (AUC = {auc_roc_mjj:.3f})",
        )
        plt.plot(
            data["fpr_mjj"][cut_middle_index_mjj],
            data["tpr_mjj"][cut_middle_index_mjj],
            "mo",
            label=f"mjj cut at {mjj_cut:.0f} GeV",
        )
        plt.plot(
            data["fpr_detajj"],
            data["tpr_detajj"],
            "g-.",
            label=f"detajj cut (AUC = {auc_roc_detajj:.3f})",
        )
        plt.plot(
            data["fpr_detajj"][cut_middle_index_detajj],
            data["tpr_detajj"][cut_middle_index_detajj],
            "go",
            label=f"detajj cut at {detajj_cut:.3f}",
        )

    plt.figure(1)
    plt.plot(
        data["fpr"][middle_thres_roc_index],
        data["tpr"][middle_thres_roc_index],
        "ko",
        label="score cut 0.5",
    )
    plt.plot([0, 1], [0, 1], "k--", label="random classifier")
    plt.title("ROC curve")
    plt.xlabel("False positive rate")
    plt.ylabel("True positive rate")
    plt.legend()
    plt.savefig(f"{outdir}/roc_curve.pdf")

    plt.figure(2)
    plt.title("Precision recall curve")
    plt.xlabel("recall")
    plt.ylabel("precision")
    plt.legend()
    plt.savefig(f"{outdir}/prc_curve.pdf")

    if simple_cut:
        # plot histograms of the mjj distribution for the
        features, predictions, _, labels = load_cache(input_dirs[0])
        d = pd.DataFrame(
            {
                "mjj": features["mjj"],
                "detajj": features["detajj"],
                "labels": labels,
                "weights": features["weight"],
                "score": predictions,
                "dataset_label" : features["dataset_label"]
            }
        )

        plot_histograms_for_each_label(
            data=d, variable="mjj", outdir=outdir, cut=mjj_cut
        )
        plot_histograms_for_each_label(
            data=d, variable="detajj", outdir=outdir, cut=detajj_cut
        )
        plot_histograms_for_each_label(data=d, variable="score", outdir=outdir, cut=0.5)

    print(f"figures saved at : {outdir}")


@cli.command()
@click.pass_context
@click.option(
    "-i",
    "--input-dir",
    required=True,
    help="path to the prediction directory in a model",
)
def plot_correl(ctx, input_dir: str) -> None:
    """
    plot the correlation plot for all variable available according to the score.
    """

    # load data
    features, predictions, label_encoding, _ = load_cache(input_dir)

    # create outdir
    outdir = pjoin(input_dir, "correlation_scatter")
    if not os.path.exists(outdir):
        os.makedirs(outdir)

    # prepare variables for table
    correl_list = []
    featname_list = []

    markersize = 100 / np.sqrt(len(predictions))
    count = 1
    # plot correlation plots
    for (featurename, feature) in features.iteritems():

        print(f"plot {featurename} ({count}/{len(features.keys())})")

        try :
            # take only the positive value of eta to find a linear correlation
            if (featurename == "leadak4_eta") or (featurename == "trailak4_eta"):
                feature = abs(feature)
                featurename = f"abs_val_{featurename}"
                
            correlation = np.corrcoef(predictions, feature)[0, 1]
            correl_list.append(correlation)
            featname_list.append(featurename)

            plt.figure()
            plt.scatter(
                predictions, feature, s=markersize, label=f"correlation = {correlation:.3f}"
            )
            plt.title(f"score correlation with {featurename}")
            plt.xlabel("Score")
            plt.ylabel(featurename)
            plt.legend()
            plt.savefig(pjoin(outdir, f"correl_{featurename}.pdf"))
            count += 1
        except :
            # Skip this invalid feature and do not calculate correlation
            print(f"no correlation for {featurename}")
            continue

    correl_list, featname_list = sort_by_pair_along_first_axis(
        correl_list, featname_list, reverse=True, sort_by_abs=True
    )

    table = {"variables": featname_list, "score_correlation": correl_list}
    table_print = tabulate(table, headers="keys")

    table_outfile = pjoin(outdir, "correlation_table.txt")
    print(f"Saving correlation data to: {table_outfile}")
    print(table_print)

    with open(table_outfile, "w") as f:
        f.write(table_print)


@cli.command()
@click.pass_context
@click.option(
    "-i",
    "--input-dir",
    required=True,
    help="path to the prediction directory in a model",
)
@click.option(
    "-c",
    "--cut-variable",
    required=False,
    default="mjj",
    help="The quantity to cut on the threshold.",
)
@click.option(
    "-t",
    "--threshold",
    type=float,
    required=False,
    default=0,
    help="The threshold/cut value.",
)
@click.option(
    "-p",
    "--plot-variable",
    default="score",
    required=False,
    help="The variable to plot the distribution after the cut",
)
@click.option(
    "-d",
    "--datasets",
    type=str,
    default=("vbf_h", "ewk_v", "qcd_v"),
    multiple=True,
    required=False,
    help="List of tags for the datasets to plot. Takes 3 possible values : 'qcd_v', 'ewk_v', 'vbf_h' ",
)
@click.option(
    "-s",
    "--save_name",
    required=False,
    type=str,
    default="",
    help="If specified, clarify the name of the pdf save file",
)
def cut_and_plot(
    ctx,
    input_dir: str,
    cut_variable: str,
    threshold: float,
    plot_variable: str,
    datasets: tuple,
    save_name: str,
) -> None:
    """
    Apply a cut on the cut-variable and plot the distribution of the plot-variable after the cut.
    The datasets to plot on the same figure can be specified via --datasets option, by default
    all three groups of datasets (if found in cache) will be plotted:
    - vbf_h, ewk_v, qcd_v
    """

    # load data
    features, predictions, _, labels = load_cache(input_dir)

    # One possible hack: Add scores to the features dictionary
    features["score"] = predictions

    # Check that dataset tags are all valid
    for data in datasets:
        assert data in [
            "vbf_h",
            "qcd_v",
            "ewk_v",
        ], f" '--dataset' doesn't accept '{data}', only  'qcd_v', 'ewk_v', 'vbf_h' are supported"

    d = pd.DataFrame(
        {
            cut_variable: features[cut_variable],
            plot_variable: features[plot_variable],
            "weights": features["weight"],
            "dataset_label": features["dataset_label"],
        }
    )

    # apply cut
    d = d[d[cut_variable] > threshold]

    # save the plot
    outdir = pjoin(input_dir, "distribution_cut")
    if not os.path.exists(outdir):
        os.makedirs(outdir)

    if threshold == 0:
        cut_title = ""
    else:
        cut_title = f"_cut_{cut_variable}_{threshold}"

    plot_histograms_for_each_label(
        data=d,
        datasets=datasets,
        variable=plot_variable,
        outdir=outdir,
        save_name=save_name,
        cut_title=cut_title,
    )

    outsave = pjoin(outdir, f"{plot_variable}_density{cut_title}{save_name}.pdf")
    print(f"(cut_and_plot) File saved at: {outsave}")


if __name__ == "__main__":
    cli()
